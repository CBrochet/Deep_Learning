{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 2 - 1.5 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Group name:</b> TARANTELLI Alix & Chloe BROCHET - LOSTIE de KERHOR - Group 12\n",
    " \n",
    " \n",
    "The aim of this session is to practice with Convolutional Neural Networks. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "\n",
    "Generate your final report (export as HTML) and upload it on the submission website http://bigfoot-m1.eurecom.fr/teachingsub/login (using your deeplearnXX/password). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed and submitted by May 30th 2018 (23:59:59 CET)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%. Can you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks,  **LeNet-5**, to go to more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell below to load the MNIST data that comes with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape: (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))\n",
    "\n",
    "epsilon = 1e-10 # this is a parameter you will use later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : My First Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example\n",
    "**y=softmax(Wx+b)** seen in the first lab. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the TensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 1.288662004\n",
      "Epoch:  02   =====> Loss= 0.732692676\n",
      "Epoch:  03   =====> Loss= 0.599985538\n",
      "Epoch:  04   =====> Loss= 0.536749699\n",
      "Epoch:  05   =====> Loss= 0.497753732\n",
      "Epoch:  06   =====> Loss= 0.471234547\n",
      "Epoch:  07   =====> Loss= 0.451261122\n",
      "Epoch:  08   =====> Loss= 0.435740518\n",
      "Epoch:  09   =====> Loss= 0.423274494\n",
      "Epoch:  10   =====> Loss= 0.413069920\n",
      "Epoch:  11   =====> Loss= 0.404554589\n",
      "Epoch:  12   =====> Loss= 0.396918491\n",
      "Epoch:  13   =====> Loss= 0.390203175\n",
      "Epoch:  14   =====> Loss= 0.384384598\n",
      "Epoch:  15   =====> Loss= 0.379245824\n",
      "Epoch:  16   =====> Loss= 0.374483929\n",
      "Epoch:  17   =====> Loss= 0.370222854\n",
      "Epoch:  18   =====> Loss= 0.366388978\n",
      "Epoch:  19   =====> Loss= 0.363033491\n",
      "Epoch:  20   =====> Loss= 0.359776941\n",
      "Epoch:  21   =====> Loss= 0.356680659\n",
      "Epoch:  22   =====> Loss= 0.354078967\n",
      "Epoch:  23   =====> Loss= 0.351193055\n",
      "Epoch:  24   =====> Loss= 0.348619844\n",
      "Epoch:  25   =====> Loss= 0.346619813\n",
      "Epoch:  26   =====> Loss= 0.344197939\n",
      "Epoch:  27   =====> Loss= 0.342365736\n",
      "Epoch:  28   =====> Loss= 0.340486971\n",
      "Epoch:  29   =====> Loss= 0.338308916\n",
      "Epoch:  30   =====> Loss= 0.336806424\n",
      "Epoch:  31   =====> Loss= 0.335083541\n",
      "Epoch:  32   =====> Loss= 0.333316851\n",
      "Epoch:  33   =====> Loss= 0.332073597\n",
      "Epoch:  34   =====> Loss= 0.330101763\n",
      "Epoch:  35   =====> Loss= 0.329191778\n",
      "Epoch:  36   =====> Loss= 0.327739827\n",
      "Epoch:  37   =====> Loss= 0.326636408\n",
      "Epoch:  38   =====> Loss= 0.325085297\n",
      "Epoch:  39   =====> Loss= 0.324172898\n",
      "Epoch:  40   =====> Loss= 0.322817892\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9156\n"
     ]
    }
   ],
   "source": [
    "#STEP 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    summary_writer.flush()\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir=lab_2/log_files/\"**\n",
    "- Click on \"Tensorboard web interface\" in Zoe  \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : The 99% MNIST Challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now more familar with **TensorFlow** and **TensorBoard**. In this section, you are to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "Then, you will make some optimizations to get more than 99% of accuracy.\n",
    "\n",
    "For more informations, have a look at this list of results: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"lenet.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 1: Lenet-5 </span></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture takes a 28x28xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1 - Convolution (5x5):** The output shape should be 28x28x6. **Activation:** ReLU. **MaxPooling:** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2 - Convolution (5x5):** The output shape should be 10x10x16. **Activation:** ReLU. **MaxPooling:** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten:** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use tf.reshape.\n",
    "\n",
    "**Layer 3 - Fully Connected:** This should have 120 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 4 - Fully Connected:** This should have 84 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 5 - Fully Connected:** This should have 10 outputs. **Activation:** softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.1 </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Here is how we proceeded to obtain a 99% accuracy : <br>\n",
    "First, we tested the model with the **SDG optimizer** on 40 epochs. Then we decided to compare the performances of this optimizer to the **Adam's optimizer**. <br>\n",
    "Second, we wanted to see the **impact of adding a dropout layer** in the model. We tested the model with both optimizers. <br>\n",
    "Third, we decided to **increase the number of epochs** to a 100 to see how this parameter increases accuracy. To do so, we had to modifiy the Loss function when using the Adam's optimizer. <br>\n",
    "**Finally, our best accuracy was achieved when using Adam's optimizer, no dropout layer and for 100 epochs.** \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions for weigths and bias initilization \n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0., shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(55000, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(55000, 28, 28, 6), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(55000, 14, 14, 6), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(55000, 10, 10, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(55000, 5, 5, 16), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(55000, 400), dtype=float32)\n",
      "Tensor(\"Relu_2:0\", shape=(55000, 120), dtype=float32)\n",
      "Tensor(\"Relu_3:0\", shape=(55000, 84), dtype=float32)\n",
      "Tensor(\"Softmax:0\", shape=(55000, 10), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(55000, 10) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def LeNet5_Model(data):    \n",
    "    # your inmplementation goes here\n",
    "    \n",
    "    # ----- Input layer -----\n",
    "    input_layer = tf.reshape(data, [-1,28, 28, 1]) \n",
    "    print(input_layer)\n",
    "    \n",
    "    # ----- Layer 1: Convolution & Activation-----\n",
    "    filter_conv1 = weight_variable([5,5,1,6])\n",
    "    strides_conv1 = [1,1,1,1]\n",
    "    biais_conv1 = bias_variable([6])\n",
    "    conv1 = tf.nn.relu(tf.nn.conv2d(input_layer, filter_conv1, strides = strides_conv1,  padding = \"SAME\") + biais_conv1 )\n",
    "    print(conv1)\n",
    " \n",
    "    # ----- Layer 1: MaxPooling -----\n",
    "    strides_maxpool1 = [1,2,2,1]\n",
    "    maxPooling1 = tf.nn.max_pool(conv1, ksize=[1,2, 2,1], strides=strides_maxpool1, padding = 'VALID')\n",
    "    print(maxPooling1)\n",
    "    \n",
    "     # ----- Layer 2: Convolution & Activation-----\n",
    "    filter_conv2 = weight_variable([5,5,6,16])\n",
    "    strides_conv2 = [1,1,1,1]\n",
    "    biais_conv2 = bias_variable([16])\n",
    "    conv2 = tf.nn.relu(tf.nn.conv2d(maxPooling1, filter_conv2, strides = strides_conv2,  padding = \"VALID\") + biais_conv2 )\n",
    "    print(conv2)\n",
    " \n",
    "    # ----- Layer 2: MaxPooling -----\n",
    "    strides_maxpool2 = [1,2,2,1]\n",
    "    maxPooling2 = tf.nn.max_pool(conv2, ksize=[1,2, 2,1], strides=strides_maxpool2, padding = 'VALID')\n",
    "    print(maxPooling2)\n",
    "    \n",
    "    # ----- Flatten -----\n",
    "    flattened_output = tf.reshape(maxPooling2,[-1,5*5*16])\n",
    "    print(flattened_output)\n",
    "    \n",
    "    # ----- Layer 3: Fully connected & activation -----\n",
    "    weight_connection3 = weight_variable([400,120])\n",
    "    biais_connection3 = bias_variable([120])\n",
    "    connection3 = tf.nn.relu(tf.matmul(flattened_output,weight_connection3)+biais_connection3)\n",
    "    print(connection3)\n",
    "    \n",
    "    # ----- Layer 4: Fully connected & activation -----\n",
    "    weight_connection4 = weight_variable([120,84])\n",
    "    biais_connection4 = bias_variable([84])\n",
    "    connection4 = tf.nn.relu(tf.matmul(connection3,weight_connection4)+biais_connection4)\n",
    "    print(connection4)\n",
    "    \n",
    "    # ----- Layer 5: Fully connected & activation -----\n",
    "    weight_connection5 = weight_variable([84,10])\n",
    "    biais_connection5 = bias_variable([10])\n",
    "    connection5 = tf.nn.softmax(tf.matmul(connection4,weight_connection5)+biais_connection5)\n",
    "    print(connection5)\n",
    "    return (connection5)\n",
    " \n",
    "    \n",
    "LeNet5_Model(X_train) \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "For the pooling layers, there is no parameters.\n",
    "\n",
    "For the convolution layers: <br>\n",
    "Weights = (K x L x D1) x F <br>\n",
    "Biaises = F\n",
    "\n",
    "__K__: filter height <br>\n",
    "__L__: filter weight <br>\n",
    "__D__: input depth <br>\n",
    "__F__: output depth <br>\n",
    "\n",
    "\n",
    "**Convolution Layer 1 :** <br>\n",
    "K = 5 <br>\n",
    "L= 5 <br>\n",
    "D = 1 <br>\n",
    "F = 6 <br>\n",
    "So, wieghts = 150, biases = 6 <br>\n",
    "\n",
    "\n",
    "**Convolution Layer 2 :** <br>\n",
    "K = 5 <br>\n",
    "L= 5 <br>\n",
    "D = 6 <br>\n",
    "F = 16 <br>\n",
    "So, wieghts = 2 400, biases = 16\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.3. </b>  Define your model, its accuracy and the loss function according to the following parameters (you can look at Section 1 to see what is expected):\n",
    "\n",
    "     Learning rate: 0.001\n",
    "     Loss Fucntion: Cross-entropy\n",
    "     Optimizer: tf.train.GradientDescentOptimizer\n",
    "     Number of epochs: 40\n",
    "     Batch size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 28, 28, 6), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 6), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 10, 10, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 5, 5, 16), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(?, 400), dtype=float32)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 120), dtype=float32)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 84), dtype=float32)\n",
      "Tensor(\"Softmax:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "logs_path = 'SDG_files/'\n",
    "saving_path ='models/'\n",
    "\n",
    "# Model, loss function and accuracy\n",
    "\n",
    "# ----- Define the model -----\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='PreData')\n",
    "model = LeNet5_Model(x)\n",
    "\n",
    "# ----- Loss function -----\n",
    "with tf.name_scope('Loss'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model), reduction_indices=1))\n",
    "   \n",
    "# ----- Accuracy -----\n",
    "acc = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "acc = tf.reduce_mean(tf.cast(acc, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(logits, labels):\n",
    "    # logits will be the outputs of your model, labels will be one-hot vectors corresponding to the actual labels\n",
    "    # logits and labels are numpy arrays\n",
    "    # this function should return the accuracy of your model\n",
    "    acc = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "    return(acc)\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    acc = evaluate(model,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss_SGD\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_SGD\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "display_step = 1\n",
    "\n",
    "with tf.name_scope('SGD'):\n",
    "        # Gradient Descent\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "def train(init, sess, logs_path, n_epochs, batch_size, optimizer, cost, merged_summary_op):\n",
    "    # optimizer and cost are the same kinds of objects as in Section 1\n",
    "    # Train your model\n",
    "    # Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "    # Print the accuracy on testing data\n",
    "    with tf.Session() as sess:\n",
    "        #train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)\n",
    "        sess.run(init)\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        saver = tf.train.Saver()\n",
    "        t1 = time()\n",
    "        print(\"Starting Training\")\n",
    "    \n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "            #print(i)\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "                #batch_xs = batch_xs.reshape(-1, 784)\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "            # Display logs per epoch step\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost), \"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "        print(\"Training Finished!\")\n",
    "        t2= time ()\n",
    "        savemodel = saver.save(sess,saving_path)\n",
    "\n",
    "        # Test model\n",
    "        # Calculate accuracy\n",
    "        print(\"Final Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}), \"Training Time\", t2-t1)\n",
    "        summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Epoch:  01   =====> Loss= 2.312004849 Accuracy: 0.1267\n",
      "Epoch:  02   =====> Loss= 2.300945116 Accuracy: 0.1447\n",
      "Epoch:  03   =====> Loss= 2.293289301 Accuracy: 0.169\n",
      "Epoch:  04   =====> Loss= 2.285715579 Accuracy: 0.1996\n",
      "Epoch:  05   =====> Loss= 2.276157175 Accuracy: 0.2432\n",
      "Epoch:  06   =====> Loss= 2.262128219 Accuracy: 0.3196\n",
      "Epoch:  07   =====> Loss= 2.238467530 Accuracy: 0.4379\n",
      "Epoch:  08   =====> Loss= 2.191733862 Accuracy: 0.515\n",
      "Epoch:  09   =====> Loss= 2.086680934 Accuracy: 0.5651\n",
      "Epoch:  10   =====> Loss= 1.821873232 Accuracy: 0.6817\n",
      "Epoch:  11   =====> Loss= 1.291416404 Accuracy: 0.7817\n",
      "Epoch:  12   =====> Loss= 0.830878836 Accuracy: 0.8318\n",
      "Epoch:  13   =====> Loss= 0.617810701 Accuracy: 0.8576\n",
      "Epoch:  14   =====> Loss= 0.514609978 Accuracy: 0.8751\n",
      "Epoch:  15   =====> Loss= 0.453673803 Accuracy: 0.8848\n",
      "Epoch:  16   =====> Loss= 0.412516177 Accuracy: 0.8944\n",
      "Epoch:  17   =====> Loss= 0.382832161 Accuracy: 0.8997\n",
      "Epoch:  18   =====> Loss= 0.359339810 Accuracy: 0.9049\n",
      "Epoch:  19   =====> Loss= 0.339848168 Accuracy: 0.9097\n",
      "Epoch:  20   =====> Loss= 0.323434081 Accuracy: 0.9141\n",
      "Epoch:  21   =====> Loss= 0.309249394 Accuracy: 0.9176\n",
      "Epoch:  22   =====> Loss= 0.296575024 Accuracy: 0.9212\n",
      "Epoch:  23   =====> Loss= 0.285306993 Accuracy: 0.924\n",
      "Epoch:  24   =====> Loss= 0.275184940 Accuracy: 0.9269\n",
      "Epoch:  25   =====> Loss= 0.265759940 Accuracy: 0.9297\n",
      "Epoch:  26   =====> Loss= 0.256775799 Accuracy: 0.9332\n",
      "Epoch:  27   =====> Loss= 0.249188202 Accuracy: 0.935\n",
      "Epoch:  28   =====> Loss= 0.241598414 Accuracy: 0.9367\n",
      "Epoch:  29   =====> Loss= 0.234542736 Accuracy: 0.9391\n",
      "Epoch:  30   =====> Loss= 0.228137478 Accuracy: 0.9405\n",
      "Epoch:  31   =====> Loss= 0.221887691 Accuracy: 0.9418\n",
      "Epoch:  32   =====> Loss= 0.216170737 Accuracy: 0.9436\n",
      "Epoch:  33   =====> Loss= 0.210440686 Accuracy: 0.9447\n",
      "Epoch:  34   =====> Loss= 0.205355137 Accuracy: 0.9467\n",
      "Epoch:  35   =====> Loss= 0.200603013 Accuracy: 0.9475\n",
      "Epoch:  36   =====> Loss= 0.195846875 Accuracy: 0.9489\n",
      "Epoch:  37   =====> Loss= 0.191483720 Accuracy: 0.9496\n",
      "Epoch:  38   =====> Loss= 0.187262281 Accuracy: 0.9505\n",
      "Epoch:  39   =====> Loss= 0.183397087 Accuracy: 0.9512\n",
      "Epoch:  40   =====> Loss= 0.179665961 Accuracy: 0.9523\n",
      "Training Finished!\n",
      "Final Accuracy: 0.9523 Training Time 627.7414333820343\n"
     ]
    }
   ],
   "source": [
    "train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.6 </b> : Use TensorBoard to visualise and save loss and accuracy curves. \n",
    "You will save figures in the folder **\"lab_2/MNIST_figures\"** and display them in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"MNIST_figures/SDG_Accuracy.JPG\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 2a: Accuracy </span></center>\n",
    "\n",
    "\n",
    "<img src=\"MNIST_figures/SDG_Loss.JPG\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 2b: Loss </span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "When we use the Stochastic Gradient Descent as optimizer, with 40 epochs, we got an accuracy 0.9523 in 627s\n",
    "It reaches an 80% accuracy really fastly then increases slowly. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Question 2.2.1 </b>\n",
    "\n",
    "- Retrain your network with AdamOptimizer and then fill the table above:\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent  |    AdamOptimizer    |\n",
    "|----------------------|--------------------|---------------------|\n",
    "| Testing Accuracy     |         0.9523     |  epoch 20: 0.989    |       \n",
    "| Training Time        |        627,74      |      626,91         | \n",
    "| Training Time / epoch|         15,69      |       15,67         |\n",
    "\n",
    "\n",
    "- Which optimizer gives the best accuracy on test data?\n",
    "<div class=\"alert alert-success\">\n",
    "With the Adam optimizer, we got a higher accuracy in fewer epochs. We faced a problem of convergence starting from the 20th epoch  because of the Loss function. Indeed, the loss is so small that taking its log is like computing log(0). To face this issue, we should add a small constant in the log function. <br>\n",
    "We notice that the accuracy is very high for the first epoch (0.97). Then it slowly increases over the next epochs. It seems that running over 40 epochs is too high and that it may be a lost of time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 28, 28, 6), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 6), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 10, 10, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 5, 5, 16), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(?, 400), dtype=float32)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 120), dtype=float32)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 84), dtype=float32)\n",
      "Tensor(\"Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "Starting Training\n",
      "Epoch:  01   =====> Loss= 0.361617278 Accuracy: 0.9701\n",
      "Epoch:  02   =====> Loss= 0.090087794 Accuracy: 0.9782\n",
      "Epoch:  03   =====> Loss= 0.062645129 Accuracy: 0.9848\n",
      "Epoch:  04   =====> Loss= 0.049700349 Accuracy: 0.9858\n",
      "Epoch:  05   =====> Loss= 0.041520815 Accuracy: 0.9876\n",
      "Epoch:  06   =====> Loss= 0.034721661 Accuracy: 0.9881\n",
      "Epoch:  07   =====> Loss= 0.029558253 Accuracy: 0.9884\n",
      "Epoch:  08   =====> Loss= 0.025184028 Accuracy: 0.9893\n",
      "Epoch:  09   =====> Loss= 0.020145406 Accuracy: 0.989\n",
      "Epoch:  10   =====> Loss= 0.017695142 Accuracy: 0.9896\n",
      "Epoch:  11   =====> Loss= 0.015584403 Accuracy: 0.9879\n",
      "Epoch:  12   =====> Loss= 0.015400420 Accuracy: 0.988\n",
      "Epoch:  13   =====> Loss= 0.013789145 Accuracy: 0.9904\n",
      "Epoch:  14   =====> Loss= 0.013054899 Accuracy: 0.9897\n",
      "Epoch:  15   =====> Loss= 0.010997909 Accuracy: 0.9907\n",
      "Epoch:  16   =====> Loss= 0.008549475 Accuracy: 0.9893\n",
      "Epoch:  17   =====> Loss= 0.009533086 Accuracy: 0.9884\n",
      "Epoch:  18   =====> Loss= 0.008320650 Accuracy: 0.9903\n",
      "Epoch:  19   =====> Loss= 0.007647281 Accuracy: 0.9899\n",
      "Epoch:  20   =====> Loss= 0.006682914 Accuracy: 0.989\n",
      "Epoch:  21   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  22   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  23   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  24   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  25   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  26   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  27   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  28   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  29   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  30   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  31   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  32   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  33   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  34   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  35   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  36   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  37   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  38   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  39   =====> Loss= nan Accuracy: 0.098\n",
      "Epoch:  40   =====> Loss= nan Accuracy: 0.098\n",
      "Training Finished!\n",
      "Final Accuracy: 0.098 Training Time 626.9135255813599\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "logs_path = 'Adam_files/'\n",
    "saving_path ='models/'\n",
    "display_step = 1\n",
    "# Model, loss function and accuracy\n",
    "\n",
    "# ----- Define the model -----\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='PrData')\n",
    "model = LeNet5_Model(x)\n",
    "\n",
    "# ----- Loss function -----\n",
    "with tf.name_scope('LossAdam'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model), reduction_indices=1))\n",
    "    \n",
    "with tf.name_scope('AccuracyAdam'):\n",
    "    acc = evaluate(model,y)\n",
    "\n",
    "\n",
    "with tf.name_scope('Adam'):\n",
    "        # Gradient Descent\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate,beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    use_locking=False,\n",
    "    name='Adam').minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "tf.summary.scalar(\"Loss_Adam\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_Adam\", acc)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.2</b> Try to add dropout (keep_prob = 0.75) before the first fully connected layer. You will use tf.nn.dropout for that purpose. What accuracy do you achieve on testing data?\n",
    "\n",
    "**Accuracy achieved on testing data:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(55000, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(55000, 28, 28, 6), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(55000, 14, 14, 6), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(55000, 10, 10, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(55000, 5, 5, 16), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(55000, 400), dtype=float32)\n",
      "Tensor(\"dropout/mul:0\", shape=(55000, 400), dtype=float32)\n",
      "Tensor(\"Relu_2:0\", shape=(55000, 120), dtype=float32)\n",
      "Tensor(\"Relu_3:0\", shape=(55000, 84), dtype=float32)\n",
      "Tensor(\"Softmax:0\", shape=(55000, 10), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(55000, 10) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def LeNet5_Model_Dropout(image):    \n",
    "    # your implementation goes here\n",
    "    # ----- Input layer -----\n",
    "    input_layer = tf.reshape(image, [-1,28, 28, 1]) \n",
    "    print(input_layer)\n",
    "    \n",
    "    # ----- Layer 1: Convolution & Activation-----\n",
    "    filter_conv1 = weight_variable([5,5,1,6])\n",
    "    strides_conv1 = [1,1,1,1]\n",
    "    biais_conv1 = bias_variable([6])\n",
    "    conv1 = tf.nn.relu(tf.nn.conv2d(input_layer, filter_conv1, strides = strides_conv1,  padding = \"SAME\") + biais_conv1 )\n",
    "    print(conv1)\n",
    " \n",
    "    # ----- Layer 1: MaxPooling -----\n",
    "    strides_maxpool1 = [1,2,2,1]\n",
    "    maxPooling1 = tf.nn.max_pool(conv1, ksize=[1,2, 2,1], strides=strides_maxpool1, padding = 'VALID')\n",
    "    print(maxPooling1)\n",
    "    \n",
    "     # ----- Layer 2: Convolution & Activation-----\n",
    "    filter_conv2 = weight_variable([5,5,6,16])\n",
    "    strides_conv2 = [1,1,1,1]\n",
    "    biais_conv2 = bias_variable([16])\n",
    "    conv2 = tf.nn.relu(tf.nn.conv2d(maxPooling1, filter_conv2, strides = strides_conv2,  padding = \"VALID\") + biais_conv2 )\n",
    "    print(conv2)\n",
    " \n",
    "    # ----- Layer 2: MaxPooling -----\n",
    "    strides_maxpool2 = [1,2,2,1]\n",
    "    maxPooling2 = tf.nn.max_pool(conv2, ksize=[1,2, 2,1], strides=strides_maxpool2, padding = 'VALID')\n",
    "    print(maxPooling2)\n",
    "    \n",
    "    # ----- Flatten -----\n",
    "    flattened_output = tf.reshape(maxPooling2,[-1,5*5*16])\n",
    "    print(flattened_output)\n",
    "    \n",
    "    #adding dropout \n",
    "    droplayer = tf.nn.dropout(flattened_output, 0.75)\n",
    "    print(droplayer)\n",
    "    \n",
    "    # ----- Layer 3: Fully connected & activation -----\n",
    "    weight_connection3 = weight_variable([400,120])\n",
    "    biais_connection3 = bias_variable([120])\n",
    "    connection3 = tf.nn.relu(tf.matmul(droplayer,weight_connection3)+biais_connection3)\n",
    "    print(connection3)\n",
    "    \n",
    "    # ----- Layer 4: Fully connected & activation -----\n",
    "    weight_connection4 = weight_variable([120,84])\n",
    "    biais_connection4 = bias_variable([84])\n",
    "    connection4 = tf.nn.relu(tf.matmul(connection3,weight_connection4)+biais_connection4)\n",
    "    print(connection4)\n",
    "    \n",
    "    # ----- Layer 5: Fully connected & activation -----\n",
    "    weight_connection5 = weight_variable([84,10])\n",
    "    biais_connection5 = bias_variable([10])\n",
    "    connection5 = tf.nn.softmax(tf.matmul(connection4,weight_connection5)+biais_connection5)\n",
    "    print(connection5)\n",
    "    return (connection5)\n",
    "\n",
    "LeNet5_Model_Dropout(X_train) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 28, 28, 6), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 6), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 10, 10, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 5, 5, 16), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(?, 400), dtype=float32)\n",
      "Tensor(\"dropout/mul:0\", shape=(?, 400), dtype=float32)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 120), dtype=float32)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 84), dtype=float32)\n",
      "Tensor(\"Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "Starting Training\n",
      "Epoch:  01   =====> Loss= 2.290595883 Accuracy: 0.1091\n",
      "Epoch:  02   =====> Loss= 2.262853019 Accuracy: 0.1642\n",
      "Epoch:  03   =====> Loss= 2.225646502 Accuracy: 0.2685\n",
      "Epoch:  04   =====> Loss= 2.156752956 Accuracy: 0.3817\n",
      "Epoch:  05   =====> Loss= 2.009450127 Accuracy: 0.4766\n",
      "Epoch:  06   =====> Loss= 1.723772571 Accuracy: 0.5599\n",
      "Epoch:  07   =====> Loss= 1.366263479 Accuracy: 0.6353\n",
      "Epoch:  08   =====> Loss= 1.101870189 Accuracy: 0.6891\n",
      "Epoch:  09   =====> Loss= 0.916699352 Accuracy: 0.7351\n",
      "Epoch:  10   =====> Loss= 0.795236989 Accuracy: 0.7705\n",
      "Epoch:  11   =====> Loss= 0.706019154 Accuracy: 0.7986\n",
      "Epoch:  12   =====> Loss= 0.639967629 Accuracy: 0.8158\n",
      "Epoch:  13   =====> Loss= 0.588125913 Accuracy: 0.8303\n",
      "Epoch:  14   =====> Loss= 0.554954310 Accuracy: 0.8439\n",
      "Epoch:  15   =====> Loss= 0.518818307 Accuracy: 0.8523\n",
      "Epoch:  16   =====> Loss= 0.491775421 Accuracy: 0.8577\n",
      "Epoch:  17   =====> Loss= 0.467978666 Accuracy: 0.8715\n",
      "Epoch:  18   =====> Loss= 0.442031823 Accuracy: 0.8697\n",
      "Epoch:  19   =====> Loss= 0.430530547 Accuracy: 0.8817\n",
      "Epoch:  20   =====> Loss= 0.412633825 Accuracy: 0.8855\n",
      "Epoch:  21   =====> Loss= 0.396153529 Accuracy: 0.8929\n",
      "Epoch:  22   =====> Loss= 0.384291985 Accuracy: 0.8904\n",
      "Epoch:  23   =====> Loss= 0.367106880 Accuracy: 0.8976\n",
      "Epoch:  24   =====> Loss= 0.358194158 Accuracy: 0.8978\n",
      "Epoch:  25   =====> Loss= 0.349041926 Accuracy: 0.9026\n",
      "Epoch:  26   =====> Loss= 0.338342304 Accuracy: 0.9085\n",
      "Epoch:  27   =====> Loss= 0.331105746 Accuracy: 0.9066\n",
      "Epoch:  28   =====> Loss= 0.323194470 Accuracy: 0.911\n",
      "Epoch:  29   =====> Loss= 0.314296361 Accuracy: 0.916\n",
      "Epoch:  30   =====> Loss= 0.307637623 Accuracy: 0.9146\n",
      "Epoch:  31   =====> Loss= 0.301448416 Accuracy: 0.9177\n",
      "Epoch:  32   =====> Loss= 0.294807192 Accuracy: 0.9209\n",
      "Epoch:  33   =====> Loss= 0.289195875 Accuracy: 0.9191\n",
      "Epoch:  34   =====> Loss= 0.279516402 Accuracy: 0.9185\n",
      "Epoch:  35   =====> Loss= 0.274128153 Accuracy: 0.9234\n",
      "Epoch:  36   =====> Loss= 0.268338115 Accuracy: 0.9241\n",
      "Epoch:  37   =====> Loss= 0.265390805 Accuracy: 0.9249\n",
      "Epoch:  38   =====> Loss= 0.260171279 Accuracy: 0.9275\n",
      "Epoch:  39   =====> Loss= 0.256903957 Accuracy: 0.9293\n",
      "Epoch:  40   =====> Loss= 0.249264045 Accuracy: 0.9284\n",
      "Training Finished!\n",
      "Final Accuracy: 0.9306 Training Time 620.433760881424\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "logs_path = 'drop_files/'\n",
    "saving_path ='models/'\n",
    "display_step = 1\n",
    "# Model, loss function and accuracy\n",
    "\n",
    "# ----- Define the model -----\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='PrData')\n",
    "model = LeNet5_Model_Dropout(x)\n",
    "\n",
    "# ----- Loss function -----\n",
    "with tf.name_scope('Lossdrop'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model), reduction_indices=1))\n",
    "    \n",
    "with tf.name_scope('Accuracydrop'):\n",
    "    acc = evaluate(model,y)\n",
    "\n",
    "with tf.name_scope('SGD'):\n",
    "        # Gradient Descent\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "        \n",
    "init = tf.global_variables_initializer()\n",
    "tf.summary.scalar(\"Loss_drop\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_drop\", acc)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "We first chose to try the model with the dropout with the SDG optimizer to fully see its impact. \n",
    "The purpose of a dropout layer is to fix the overfitting problem. \n",
    "As some nodes are droped it is also supposed to be faster. It is 7 seconds faster. \n",
    "Yet, the final accurancy is lower than without using the dropout layer (0.93 vs 0.95)\n",
    "That may be because our training set is really similar and overfitting is the solution\n",
    "With this training set, using the model without a dropout is more efficient. \n",
    "<\\div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 28, 28, 6), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 6), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 10, 10, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 5, 5, 16), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(?, 400), dtype=float32)\n",
      "Tensor(\"dropout/mul:0\", shape=(?, 400), dtype=float32)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 120), dtype=float32)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 84), dtype=float32)\n",
      "Tensor(\"Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "Starting Training\n",
      "Epoch:  01   =====> Loss= 0.392393920 Accuracy: 0.9629\n",
      "Epoch:  02   =====> Loss= 0.113279319 Accuracy: 0.9748\n",
      "Epoch:  03   =====> Loss= 0.082501368 Accuracy: 0.9808\n",
      "Epoch:  04   =====> Loss= 0.067433696 Accuracy: 0.9803\n",
      "Epoch:  05   =====> Loss= 0.057805305 Accuracy: 0.9846\n",
      "Epoch:  06   =====> Loss= 0.052207252 Accuracy: 0.9842\n",
      "Epoch:  07   =====> Loss= 0.045424073 Accuracy: 0.985\n",
      "Epoch:  08   =====> Loss= 0.042473145 Accuracy: 0.9854\n",
      "Epoch:  09   =====> Loss= 0.037583204 Accuracy: 0.985\n",
      "Epoch:  10   =====> Loss= 0.033600830 Accuracy: 0.9868\n",
      "Epoch:  11   =====> Loss= 0.034474282 Accuracy: 0.9864\n",
      "Epoch:  12   =====> Loss= 0.031571014 Accuracy: 0.9861\n",
      "Epoch:  13   =====> Loss= 0.027099294 Accuracy: 0.9872\n",
      "Epoch:  14   =====> Loss= 0.026540695 Accuracy: 0.9875\n",
      "Epoch:  15   =====> Loss= 0.024904305 Accuracy: 0.9878\n",
      "Epoch:  16   =====> Loss= 0.024117591 Accuracy: 0.9875\n",
      "Epoch:  17   =====> Loss= 0.022143408 Accuracy: 0.9867\n",
      "Epoch:  18   =====> Loss= 0.020199066 Accuracy: 0.9868\n",
      "Epoch:  19   =====> Loss= 0.021516394 Accuracy: 0.9888\n",
      "Epoch:  20   =====> Loss= 0.018625822 Accuracy: 0.9881\n",
      "Training Finished!\n",
      "Final Accuracy: 0.9879 Training Time 315.11270403862\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 128\n",
    "logs_path = 'Adam_drop_files/'\n",
    "saving_path ='models/'\n",
    "display_step = 1\n",
    "# Model, loss function and accuracy\n",
    "\n",
    "# ----- Define the model -----\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='PrData')\n",
    "model = LeNet5_Model_Dropout(x)\n",
    "\n",
    "# ----- Loss function -----\n",
    "with tf.name_scope('Loss_drop_adam'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model), reduction_indices=1))\n",
    "    \n",
    "with tf.name_scope('Accuracy_drop_adam'):\n",
    "    acc = evaluate(model,y)\n",
    "\n",
    "with tf.name_scope('Adamdrop'):\n",
    "        # Gradient Descent\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "        \n",
    "init = tf.global_variables_initializer()\n",
    "tf.summary.scalar(\"Loss_drop\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_drop\", acc)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**Evaluating the impact of the nb of epochs** <br>\n",
    "As 40 epochs were not enough to reach a 99% accuracy, let's try with the following parameters : nb of epochs 100, optimizer : SDG\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 28, 28, 6), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 6), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 10, 10, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 5, 5, 16), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(?, 400), dtype=float32)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 120), dtype=float32)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 84), dtype=float32)\n",
      "Tensor(\"Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "Starting Training\n",
      "Epoch:  10   =====> Loss= 0.424216607 Accuracy: 0.8886\n",
      "Epoch:  20   =====> Loss= 0.272833335 Accuracy: 0.9263\n",
      "Epoch:  30   =====> Loss= 0.212675605 Accuracy: 0.9432\n",
      "Epoch:  40   =====> Loss= 0.177034984 Accuracy: 0.9544\n",
      "Epoch:  50   =====> Loss= 0.152907702 Accuracy: 0.9596\n",
      "Epoch:  60   =====> Loss= 0.135107857 Accuracy: 0.9643\n",
      "Epoch:  70   =====> Loss= 0.121158943 Accuracy: 0.9676\n",
      "Epoch:  80   =====> Loss= 0.110109265 Accuracy: 0.9706\n",
      "Epoch:  90   =====> Loss= 0.101161493 Accuracy: 0.9721\n",
      "Epoch:  100   =====> Loss= 0.093676991 Accuracy: 0.9733\n",
      "Training Finished!\n",
      "Final Accuracy: 0.9733 Training Time 1419.1857075691223\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 128\n",
    "logs_path = 'SDG100_files/'\n",
    "saving_path ='models/'\n",
    "display_step = 10\n",
    "# Model, loss function and accuracy\n",
    "\n",
    "# ----- Define the model -----\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='PrData')\n",
    "model = LeNet5_Model(x)\n",
    "\n",
    "# ----- Loss function -----\n",
    "with tf.name_scope('Lossdrop'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model), reduction_indices=1))\n",
    "    \n",
    "with tf.name_scope('Accuracydrop'):\n",
    "    acc = evaluate(model,y)\n",
    "\n",
    "with tf.name_scope('SGD'):\n",
    "        # Gradient Descent\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "        \n",
    "init = tf.global_variables_initializer()\n",
    "tf.summary.scalar(\"Loss_100epoch\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_100epoch\", acc)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"MNIST_figures/SDG100_Accuracy.JPG\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 3a: Accuracy, SDG, 100 epochs </span></center>\n",
    "\n",
    "\n",
    "<img src=\"MNIST_figures/SDG100_Loss.JPG\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 3b: Loss, SDG, 100 epochs </span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**Evaluating the impact of the nb of epochs** <br>\n",
    "Increasing the number of epochs increases the accuracy from 0.95 to 0.973. \n",
    "Yet, that doesn't seem to be a great improvement as a better accuracy as achieved using Adam's optimizer with only 3 epochs. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Let's try and fix the stability issue of the Loss function when using the Adam optimizer**  <br>\n",
    "This instability is due to the fact that the loss is so close to 0 the algorithm applies wrong optimizations. \n",
    "Then, we need to modify the Loss function by adding a small constant <br>\n",
    "__cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model+0.001), reduction_indices=1))__\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 28, 28, 6), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 6), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 10, 10, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 5, 5, 16), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(?, 400), dtype=float32)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 120), dtype=float32)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 84), dtype=float32)\n",
      "Tensor(\"Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "Starting Training\n",
      "Epoch:  01   =====> Loss= 0.349506255 Accuracy: 0.9657\n",
      "Epoch:  02   =====> Loss= 0.095894996 Accuracy: 0.9783\n",
      "Epoch:  03   =====> Loss= 0.068343517 Accuracy: 0.9824\n",
      "Epoch:  04   =====> Loss= 0.053774344 Accuracy: 0.9871\n",
      "Epoch:  05   =====> Loss= 0.045449615 Accuracy: 0.9864\n",
      "Epoch:  06   =====> Loss= 0.038054175 Accuracy: 0.9844\n",
      "Epoch:  07   =====> Loss= 0.031517917 Accuracy: 0.9856\n",
      "Epoch:  08   =====> Loss= 0.026478725 Accuracy: 0.9863\n",
      "Epoch:  09   =====> Loss= 0.023710335 Accuracy: 0.9884\n",
      "Epoch:  10   =====> Loss= 0.020200271 Accuracy: 0.9873\n",
      "Epoch:  11   =====> Loss= 0.017016989 Accuracy: 0.9874\n",
      "Epoch:  12   =====> Loss= 0.015402614 Accuracy: 0.989\n",
      "Epoch:  13   =====> Loss= 0.014884858 Accuracy: 0.9891\n",
      "Epoch:  14   =====> Loss= 0.012927235 Accuracy: 0.9904\n",
      "Epoch:  15   =====> Loss= 0.012076609 Accuracy: 0.9904\n",
      "Epoch:  16   =====> Loss= 0.011471565 Accuracy: 0.9904\n",
      "Epoch:  17   =====> Loss= 0.010260078 Accuracy: 0.989\n",
      "Epoch:  18   =====> Loss= 0.009299751 Accuracy: 0.9906\n",
      "Epoch:  19   =====> Loss= 0.010296477 Accuracy: 0.9856\n",
      "Epoch:  20   =====> Loss= 0.009971270 Accuracy: 0.99\n",
      "Epoch:  21   =====> Loss= 0.007763124 Accuracy: 0.9884\n",
      "Epoch:  22   =====> Loss= 0.007480815 Accuracy: 0.9893\n",
      "Epoch:  23   =====> Loss= 0.005835343 Accuracy: 0.9881\n",
      "Epoch:  24   =====> Loss= 0.004790389 Accuracy: 0.9897\n",
      "Epoch:  25   =====> Loss= 0.006753969 Accuracy: 0.989\n",
      "Epoch:  26   =====> Loss= 0.004875692 Accuracy: 0.9891\n",
      "Epoch:  27   =====> Loss= 0.006491990 Accuracy: 0.9885\n",
      "Epoch:  28   =====> Loss= 0.004770237 Accuracy: 0.9898\n",
      "Epoch:  29   =====> Loss= 0.004606898 Accuracy: 0.989\n",
      "Epoch:  30   =====> Loss= 0.005070477 Accuracy: 0.991\n",
      "Epoch:  31   =====> Loss= 0.004541688 Accuracy: 0.9891\n",
      "Epoch:  32   =====> Loss= 0.004253525 Accuracy: 0.9893\n",
      "Epoch:  33   =====> Loss= 0.004567377 Accuracy: 0.9902\n",
      "Epoch:  34   =====> Loss= 0.003831542 Accuracy: 0.9879\n",
      "Epoch:  35   =====> Loss= 0.006338953 Accuracy: 0.9901\n",
      "Epoch:  36   =====> Loss= 0.003661392 Accuracy: 0.9907\n",
      "Epoch:  37   =====> Loss= 0.003803045 Accuracy: 0.99\n",
      "Epoch:  38   =====> Loss= 0.004002899 Accuracy: 0.9914\n",
      "Epoch:  39   =====> Loss= 0.004804230 Accuracy: 0.9903\n",
      "Epoch:  40   =====> Loss= 0.002737796 Accuracy: 0.9912\n",
      "Training Finished!\n",
      "Final Accuracy: 0.9912 Training Time 624.8860087394714\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "logs_path = 'AdamLoss_files/'\n",
    "saving_path ='models/'\n",
    "display_step = 1\n",
    "# Model, loss function and accuracy\n",
    "\n",
    "# ----- Define the model -----\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='PrData')\n",
    "model = LeNet5_Model(x)\n",
    "\n",
    "# ----- Loss function -----\n",
    "with tf.name_scope('LossAdam2'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model+0.001), reduction_indices=1))\n",
    "    \n",
    "with tf.name_scope('AccuracyAdam'):\n",
    "    acc = evaluate(model,y)\n",
    "\n",
    "\n",
    "with tf.name_scope('Adam'):\n",
    "        # Gradient Descent\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate,beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    use_locking=False,\n",
    "    name='Adam').minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "tf.summary.scalar(\"Loss_Adam\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_Adam\", acc)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MNIST_figures/Adam_Accuracy.JPG\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 4a: Adam Accuracy </span></center>\n",
    "\n",
    "\n",
    "<img src=\"MNIST_figures/Adam_Loss.JPG\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 4b: Adam Loss </span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Then, with the loss function modified, with 40 epochs and the Adam's optimizer we reach an **accuracy of 0.9912** ! It is the best model so far in terms of accuracy and time performances.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**Let's now use the following parameters** <br>\n",
    "optimizer : Adam <br>\n",
    "epochs : 100 <br>\n",
    "dropout layer : yes <br>\n",
    "with the loss function modified\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 28, 28, 6), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 6), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 10, 10, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 5, 5, 16), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(?, 400), dtype=float32)\n",
      "Tensor(\"dropout/mul:0\", shape=(?, 400), dtype=float32)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 120), dtype=float32)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 84), dtype=float32)\n",
      "Tensor(\"Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "Starting Training\n",
      "Epoch:  10   =====> Loss= 0.033881307 Accuracy: 0.9859\n",
      "Epoch:  20   =====> Loss= 0.018523285 Accuracy: 0.9897\n",
      "Epoch:  30   =====> Loss= 0.013141273 Accuracy: 0.9891\n",
      "Epoch:  40   =====> Loss= 0.010596964 Accuracy: 0.9884\n",
      "Epoch:  50   =====> Loss= 0.007855424 Accuracy: 0.9904\n",
      "Epoch:  60   =====> Loss= 0.007567481 Accuracy: 0.9895\n",
      "Epoch:  70   =====> Loss= 0.007358544 Accuracy: 0.991\n",
      "Epoch:  80   =====> Loss= 0.006445506 Accuracy: 0.9906\n",
      "Epoch:  90   =====> Loss= 0.005861435 Accuracy: 0.9903\n",
      "Epoch:  100   =====> Loss= 0.004658667 Accuracy: 0.99\n",
      "Training Finished!\n",
      "Final Accuracy: 0.988 Training Time 1455.9771184921265\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 128\n",
    "logs_path = 'Adam100_files/'\n",
    "saving_path ='models/'\n",
    "display_step = 10\n",
    "# Model, loss function and accuracy\n",
    "\n",
    "# ----- Define the model -----\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InData')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='PrData')\n",
    "model = LeNet5_Model_Dropout(x)\n",
    "\n",
    "# ----- Loss function -----\n",
    "with tf.name_scope('LossAdam100'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model+0.001), reduction_indices=1))\n",
    "    \n",
    "with tf.name_scope('AccuracyAdam100'):\n",
    "    acc = evaluate(model,y)\n",
    "\n",
    "\n",
    "with tf.name_scope('Adam'):\n",
    "        # Gradient Descent\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate,beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    use_locking=False,\n",
    "    name='Adam').minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "tf.summary.scalar(\"Loss_Adam\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_Adam\", acc)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MNIST_figures/Adam100_Accuracy.JPG\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 5a: Adam Accuracy </span></center>\n",
    "\n",
    "\n",
    "<img src=\"MNIST_figures/Adam100_Loss.JPG\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 5b: Adam Loss </span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Then, we obtained an Accuracy of 0.988 with the last model (optimizer = Adam, nb epoch = 100, Loss function modified). \n",
    "Again, the dropout layer does not present any improvement on the training set. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 28, 28, 6), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 6), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 10, 10, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 5, 5, 16), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(?, 400), dtype=float32)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 120), dtype=float32)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 84), dtype=float32)\n",
      "Tensor(\"Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "Starting Training\n",
      "Epoch:  10   =====> Loss= 0.019452982 Accuracy: 0.9879\n",
      "Epoch:  20   =====> Loss= 0.009337226 Accuracy: 0.9895\n",
      "Epoch:  30   =====> Loss= 0.005278638 Accuracy: 0.9902\n",
      "Epoch:  40   =====> Loss= 0.001642457 Accuracy: 0.9911\n",
      "Epoch:  50   =====> Loss= 0.004150455 Accuracy: 0.9903\n",
      "Epoch:  60   =====> Loss= 0.004520883 Accuracy: 0.9909\n",
      "Epoch:  70   =====> Loss= 0.001729269 Accuracy: 0.9918\n",
      "Epoch:  80   =====> Loss= 0.003938103 Accuracy: 0.9914\n",
      "Epoch:  90   =====> Loss= 0.001266730 Accuracy: 0.9918\n",
      "Epoch:  100   =====> Loss= 0.001265438 Accuracy: 0.9924\n",
      "Training Finished!\n",
      "Final Accuracy: 0.9924 Training Time 1751.4814393520355\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 128\n",
    "logs_path = 'Adam100_files/'\n",
    "saving_path ='models/'\n",
    "display_step = 10\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='DataIn')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='PrData')\n",
    "model = LeNet5_Model(x)\n",
    "\n",
    "# ----- Loss function -----\n",
    "with tf.name_scope('LossAdam100nodrop'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model+0.001), reduction_indices=1))\n",
    "    \n",
    "with tf.name_scope('AccuracyAdam100nodrop'):\n",
    "    acc = evaluate(model,y)\n",
    "\n",
    "\n",
    "with tf.name_scope('Adam'):\n",
    "        # Gradient Descent\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate,beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    use_locking=False,\n",
    "    name='Adam').minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "tf.summary.scalar(\"Loss_Adam\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy_Adam\", acc)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**Using the following parameters** <br>\n",
    "optimizer : Adam <br>\n",
    "epochs : 100 <br>\n",
    "dropout layer : no <br>\n",
    "with the loss function modified\n",
    "**we reached an accuracy of 0.9924**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
